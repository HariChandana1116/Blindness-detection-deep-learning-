{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3LR62PO9OYe"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "from itertools import chain\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Torch: {torch.__version__}\")"
      ],
      "metadata": {
        "id": "WDAtuOBd9mUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "lr = 5e-4\n",
        "gamma = 0.8\n",
        "seed = 42\n",
        "num_classes = 1\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "_tpNQflX9mKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)"
      ],
      "metadata": {
        "id": "v8C7FqUU9l9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same preprocesssing that I used to create the dataset. This will be used on the training data before we submit to the leaderboard"
      ],
      "metadata": {
        "id": "RCZHbyFO-AJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The Code from: https://www.kaggle.com/ratthachat/aptos-updated-albumentation-meets-grad-cam\n",
        "import cv2\n",
        "\n",
        "def crop_image1(img,tol=7):\n",
        "    # img is image data\n",
        "    # tol  is tolerance\n",
        "        \n",
        "    mask = img>tol\n",
        "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "\n",
        "def crop_image_from_gray(img,tol=7):\n",
        "    if img.ndim ==2:\n",
        "        mask = img>tol\n",
        "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "    elif img.ndim==3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mask = gray_img>tol\n",
        "        \n",
        "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
        "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
        "            return img # return original image\n",
        "        else:\n",
        "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
        "    #         print(img1.shape,img2.shape,img3.shape)\n",
        "            img = np.stack([img1,img2,img3],axis=-1)\n",
        "    #         print(img.shape)\n",
        "        return img"
      ],
      "metadata": {
        "id": "Bca5k2yS97bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start by transforming the test images in the same way as the training images."
      ],
      "metadata": {
        "id": "2QR-kcZg-UXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inPath = '../input/aptos2019-blindness-detection/test_images'\n",
        "  \n",
        "# path of the folder that will contain the modified image\n",
        "try:\n",
        "    os.mkdir(\"test_images_transformed\")\n",
        "except:\n",
        "    print(\"path already exists\")\n",
        "\n",
        "outPath =\"test_images_transformed\"\n",
        "\n",
        "for imagePath in tqdm(os.listdir(inPath)):\n",
        "    # imagePath contains name of the image \n",
        "    inputPath = os.path.join(inPath, imagePath)\n",
        "\n",
        "    image = cv2.imread(inputPath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = crop_image_from_gray(image)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    image = cv2.addWeighted (image,4, cv2.GaussianBlur( image , (0,0) , 30) ,-4 ,128)\n",
        "\n",
        "    fullOutPath = os.path.join(outPath, imagePath)\n",
        "    cv2.imwrite(fullOutPath, image)"
      ],
      "metadata": {
        "id": "9Sq9nB8Q97P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '../input/custom-transform-blindness-2019/train_images_transformed'\n",
        "test_dir = './test_images_transformed'"
      ],
      "metadata": {
        "id": "fT5s7y4397Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = glob.glob(os.path.join(train_dir,'*.*'))\n",
        "test_list = glob.glob(os.path.join(test_dir, '*.png'))"
      ],
      "metadata": {
        "id": "lkqk70La-xsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Data: {len(train_list)}\")\n",
        "print(f\"Test Data: {len(test_list)}\")"
      ],
      "metadata": {
        "id": "9qkwfcA_-xh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading the labels"
      ],
      "metadata": {
        "id": "3iSXnDBf_gGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
        "df_train_old = pd.read_csv(\"../input/resized-2015-2019-blindness-detection-images/labels/trainLabels15.csv\")\n",
        "df_train_old = df_train_old.rename({\"image\" : \"id_code\", \"level\" : \"diagnosis\"}, axis=1)\n",
        "df_train = df_train.append(df_train_old).reset_index(drop=True)\n",
        "\n",
        "labels = df_train['diagnosis'].values\n",
        "label_lookup = df_train.set_index('id_code')\n",
        "\n",
        "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')"
      ],
      "metadata": {
        "id": "dLMfH902_sBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = df_train['diagnosis'].value_counts()\n",
        "dfs = [df_train[df_train['diagnosis'] == i].sample(class_weights[4]) for i in range(5)]\n",
        "resampled = pd.concat(dfs, axis = 0)"
      ],
      "metadata": {
        "id": "GJmXtXap_thc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled.diagnosis.value_counts()"
      ],
      "metadata": {
        "id": "wZr7B2DL_seQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_list = (train_dir + '/' + resampled['id_code'].apply(lambda x: x + ('.jpg' if '_' in x else '.png'))).values\n",
        "new_train_list"
      ],
      "metadata": {
        "id": "63NZ4-yeAEsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoding"
      ],
      "metadata": {
        "id": "38HTwFRrAMtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = pd.get_dummies(df_train['diagnosis']).values\n",
        "\n",
        "print(y_train.shape)\n",
        "y_train_multi = np.empty(y_train.shape, dtype=y_train.dtype)\n",
        "y_train_multi[:, 4] = y_train[:, 4]\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    y_train_multi[:, i] = np.logical_or(y_train[:, i], y_train_multi[:, i+1])\n",
        "\n",
        "print(\"Original y_train:\", y_train.sum(axis=0))\n",
        "print(\"Multilabel version:\", y_train_multi.sum(axis=0))"
      ],
      "metadata": {
        "id": "Omyj4DxqAEe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_multi"
      ],
      "metadata": {
        "id": "u_jFLAaFAUVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_index = lambda x : df_train[df_train.id_code == x].index[0]\n",
        "y_train_multi[get_index('0a4e1a29ffff')]"
      ],
      "metadata": {
        "id": "TlMJwfDtAVDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Validaition split. This validation is only used to monitor the model's performance. The true test set for the leaderboard is a secret, and will be run without us having access to it."
      ],
      "metadata": {
        "id": "59P2xLVHA-Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_list, valid_list = train_test_split(new_train_list, test_size=0.05, random_state=seed)"
      ],
      "metadata": {
        "id": "Pj8dBhlbBAWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_list))\n",
        "print(len(valid_list))"
      ],
      "metadata": {
        "id": "J8UFgUlUBN4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "#         transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "cuZ_VUGXBSl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Blindness2019(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        label = label_lookup.loc[img_path.split(\"/\")[-1].split(\".\")[0]][0]\n",
        "#         label = torch.tensor(label).to(torch.float32)\n",
        "        image_id = img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "#         label = y_train_multi[get_index(image_id)]\n",
        "#         label = y_train_multi[random.randint(0,3000)]\n",
        "        return img_transformed, label\n",
        "\n",
        "class Blindness2019Test(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "            \n",
        "        return img_transformed"
      ],
      "metadata": {
        "id": "-CL_mXUzBN0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Blindness2019(train_list, transform=train_transforms)\n",
        "valid_data = Blindness2019(valid_list, transform=test_transforms)\n",
        "test_data = Blindness2019Test(test_list, transform=test_transforms)"
      ],
      "metadata": {
        "id": "Mp8Wu6F-BkS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating data loader with the batch size"
      ],
      "metadata": {
        "id": "NNq7jiytBvud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "zMh0R6WiBqNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(train_loader))\n",
        "print(len(valid_data), len(valid_loader))"
      ],
      "metadata": {
        "id": "YMvgmcRvBqq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment one of the below cells to pick the model you want to run\n",
        "All the models are in the same notebook, but it would take too long to run them all. So instead, pick one, and uncomment it! This will be the model variable that the training loop uses"
      ],
      "metadata": {
        "id": "pDlciiWMCZuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficientnet"
      ],
      "metadata": {
        "id": "L9-YiBNoB_a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path = [\n",
        "    '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master',\n",
        "] + sys.path\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "model = EfficientNet.from_name('efficientnet-b0')\n",
        "model.load_state_dict(torch.load('../input/efficientnet-pytorch/efficientnet-b0-08094119.pth'))\n",
        "in_features = model._fc.in_features\n",
        "model._fc = nn.Linear(in_features, 5)\n",
        "# model.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "# model._fc = nn.Sequential(\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(in_features=in_features, out_features=128, bias=True),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(in_features=128, out_features=5, bias=True, ),\n",
        "#                 nn.Sigmoid()\n",
        "#             )\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "pfG89DlKBSsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ../input/vit-pytorch/Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "! pip install ../input/vit-pytorch/einops-0.3.2-py3-none-any.whl\n",
        "! pip install ../input/vit-pytorch/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
        "! pip install ../input/vit-pytorch/torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl\n",
        "! pip install ../input/vit-pytorch/torchvision-0.11.1-cp37-cp37m-manylinux1_x86_64.whl\n",
        "! pip install ../input/vit-pytorch/typing_extensions-4.0.1-py3-none-any.whl\n",
        "! pip install ../input/vit-pytorch/vit_pytorch-0.24.3-py3-none-any.whl\n",
        "from vit_pytorch.efficient import ViT\n",
        "\n",
        "! pip install ../input/linformer/linformer-0.2.1-py3-none-any.whl\n",
        "! pip install ../input/linformer/torch-1.10.0-cp37-cp37m-manylinux1_x86_64 (1).whl\n",
        "! pip install ../input/linformer/typing_extensions-4.0.1-py3-none-any (1).whl\n",
        "from linformer import Linformer"
      ],
      "metadata": {
        "id": "SMkB-JXnBNxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linformer ViT"
      ],
      "metadata": {
        "id": "BEJ4QL4wC2T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "efficient_transformer = Linformer(\n",
        "    dim=128,\n",
        "    seq_len=49+1,  # 7x7 patches + 1 cls-token\n",
        "    depth=32,\n",
        "    heads=16,\n",
        "    k=64\n",
        ")\n",
        "\n",
        "v = ViT(\n",
        "    dim=32,\n",
        "    image_size=224,\n",
        "    patch_size=16,\n",
        "    num_classes=5,\n",
        "    transformer=efficient_transformer,\n",
        "    channels=3,\n",
        ")\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#                 v,\n",
        "# #                 nn.Dropout(p=.5),\n",
        "#                 nn.Linear(in_features=256, out_features=128, bias=True),\n",
        "#                 nn.Linear(in_features=128, out_features=5, bias=True, ),\n",
        "#                 nn.Sigmoid()\n",
        "#             ).to(device)"
      ],
      "metadata": {
        "id": "47QoyFSbCv3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep ViT"
      ],
      "metadata": {
        "id": "pKxxDUWADE_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from vit_pytorch.deepvit import DeepViT\n",
        "\n",
        "# v = DeepViT(\n",
        "#     image_size = 224,\n",
        "#     patch_size = 32,\n",
        "#     num_classes = 256,\n",
        "#     dim = 1024,\n",
        "#     depth = 6,\n",
        "#     heads = 16,\n",
        "#     mlp_dim = 2048,\n",
        "#     dropout = 0.1,\n",
        "#     emb_dropout = 0.1\n",
        "# ).to(device)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#                 v,\n",
        "# #                 nn.Dropout(p=.5),\n",
        "#                 nn.Linear(in_features=256, out_features=128, bias=True),\n",
        "#                 nn.Linear(in_features=128, out_features=5, bias=True, ),\n",
        "#                 nn.Sigmoid()\n",
        "#             ).to(device)"
      ],
      "metadata": {
        "id": "x1U91FbPC46o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}